<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="COOPERA">
  <meta property="og:title" content="COOPERA"/>
  <meta property="og:description" content="COOPERA"/>
  <meta property="og:url" content="https://dannymcy.github.io/coopera/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="sliders/illum_car_0.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>COOPERA</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">COOPERA: Continual Open-Ended<br>Human-Robot Assistance</h1>
              <h2> <font size="+2.5"> NeurIPS 2025 (Spotlight) </font></h1>
                <br>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://dannymcy.github.io" target="_blank">Chenyang Ma</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://kailucs.github.io/" target="_blank">Kai Lu</a><sup>1</sup>,</span>
                    </br>
                    <span class="author-block">
                      <a href="https://rutadesai.github.io/" target="_blank">Ruta Desai</a><sup>†</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.xavierpuigf.com/" target="_blank">Xavier Puig</a><sup>†</sup>,</span>
                        <span class="author-block">
                          <a href="https://www.cs.ox.ac.uk/people/andrew.markham/" target="_blank">Andrew Markham</a><sup>1†</sup>,</span>
                          <span class="author-block">
                            <a href="https://en.wikipedia.org/wiki/Niki_Trigoni" target="_blank">Niki Trigoni</a><sup>1†</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="margin-top: 5px;"> 
                      <sup>1</sup> University of Oxford<br>
                      <sup>†</sup> equal advising
                    </span>
                    <!-- <span class="author-block" style="margin-top: 5px;"> <sup>1</sup> University of Oxford<br><sup>2</sup> FAIR, Meta<br></span> -->
                  </div>
                  <br>
                  

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="Zero_Shot_Task_Hallucination.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2510.23495" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <span class="link-block">
                    <a href="https://github.com/dannymcy/coopera_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4"> -->
        <!-- <video controls autoplay muted loop>
          <source src="static/videos/teaser_vid.mp4" type="video/mp4"> -->
        </video>
          <img src="static/images/teaser.jpg" alt="COOPERA"/>

          <!-- <video controls loop>
            <source src="static/images/teaser_vid.mov">
            Your browser does not support the video tag.
          </video> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <b>TL;DR:</b> We introduce COOPERA, a framework for studying continual, open-ended human-robot collaboration. 
        COOPERA includes (a) simulated humans driven by psychological traits and long-term intentions,
        (b) continuous human feedback, 
        and (c) a benchmark and approach to personalize the robot's collaborative actions.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- YouTube Video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe width="100%" height="400" 
              src="https://www.youtube.com/embed/vgMbFhCkh9c" 
              frameborder="0" 
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
              allowfullscreen>
            </iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. 
            However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and 
            lack a human model to learn from. This work introduces <b>COOPERA</b>, a novel framework for <b>CO</b>ntinual, 
            <b>OP</b>en-<b>E</b>nded human-<b>R</b>obot <b>A</b>ssistance, where simulated humans, driven by psychological traits 
            and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, 
            for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks 
            across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions 
            by learning human traits and context-dependent intents. Experiments validate the realism of our simulated humans and demonstrate the value 
            of inferring and personalizing to human intents for open-ended and long-term HRC.          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->

<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Framework Overview</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/framework.jpg" alt="Framework Overview"/>

           <div class="content has-text-justified">
             <p>
              Our goal is to enable the study of continual HRC in open-ended tasks. 
              To that end, we investigate how a robotic agent can become more effective in assisting humans by learning from their behavior. 
              Central to COOPERA are LLM-powered simulated humans driven by traits and long-term intentions that the robot can reason for effective collaboration, 
              and a human feedback mechanism for improving collaboration over time. 
              COOPERA offers different collaborative tasks across various time-scales. 
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Simulating Humans</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/human_simulation.jpg" alt="Human Simulation"/>

           <div class="content has-text-justified">
             <p>
              We aim to model humans who interact in the environment over long periods of time, act driven by their goals, preferences, and context, 
              and who can react and provide feedback as a robot assists them. To achieve this, 
              we propose a hierarchical model that combines LLMs and 3D human motion to simulate long-term, 
              realistic human behaviors in indoor environments.
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Instantiating COOPERA with an Assistive Agent</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/method.jpg" alt="Building an Assistive Agent"/>

           <div class="content has-text-justified">
             <p>
              To study COOPERA, we propose an approach for continual HRC, alongside benchmark methods. Our approach enables the robot to improve its assistive performance by learning
              correlations between human intentions, tasks, traits, and temporal dependencies at each time of day. 
             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->







<section class="hero is-small" style="margin-top: 25px; margin-bottom: 25px;">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <div class="content has-text-justified">
             <!-- <p>
              We provide an extensive empirical study combining multiple off-the-shelf and handcrafted
              datasets, ranging from fundamental spatial questions regarding relative positions and orientations to providing fine-grained 3D information
              on objects' locations, sizes, inclinations, and
              dynamic changes, and plan for robotics tasks
              with full 3D trajectories.
             </p> -->

              <!-- Experiment 1. -->
              <h2 class="title is-4">Simulated Humans</h2>
                <!-- <p>
                  We experiment on the basic form of spatial VQA introduced by SpatialVLM, Intra-Image Object Relations VQA (IaOR-VQA), 
                  as well as two new forms introduced by us: Intra-Image Angular Discrepancies VQA (IaAD-VQA) and Inter-Image Spatial Dynamics VQA (IrSD-VQA).

                  <br><br>In the figure below, we list some sample question and answer pairs generated by our pipeline.
                </p> -->
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                   <img src="static/images/human_demo.jpg" alt="Simulated Humans"  style="width: 100%; height: auto;" />
                </div>
              </div>

              <!-- Experiment 2. -->
              <h2 class="title is-4">Human-Robot Collaboration</h2>
                <!-- <p>
                  By partially reconstructing the 3D scene with visual alignments, our framework enables VLMs to use
                  tools like rapidly-exploring random tree star (RRT*) to generate accurate, collision-free paths
                  based on task specifications. Given a robot's egocentric observation of a scene with
                  multiple objects, our pipeline uses traditional planning to solve robotics pick-and-stack.
                </p> -->
              <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                   <img src="static/images/HRC.jpg" alt="Human-Robot Collaboration"/>
                </div>
              </div>


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX"  style="margin-top: 25px; margin-bottom: 25px;">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@inproceedings{ma2024COOPERA,
  title={COOPERA: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors},
  author={Ma, Chenyang and Lu, Kai and Cheng, Ta-Ying and Trigoni, Niki and Markham, Andrew},
  booktitle={Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}</code></pre> -->
<pre><code>@inproceedings{ma2025coopera,
  title={COOPERA: Continual Open-Ended Human-Robot Assistance},
  author={Ma, Chenyang and Lu, Kai and Desai, Ruta and Puig, Xavier and Markham, Andrew and Trigoni, Niki},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
